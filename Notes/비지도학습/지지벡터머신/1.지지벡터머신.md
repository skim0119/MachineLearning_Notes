# 지지벡터 머신(Support Vector Machine, SVM)

## 지지벡터 머신 역사와 이론

![svm(unlicensed)](https://docs.opencv.org/2.4/_images/optimal-hyperplane.png)

지지벡터 머신은 상당히 짧은 역사를 가지고 있습니다. 이론을 확립하고 제시한 사람이 아직도 살아계시니 말이죠. 그럼에도 불구하고 아마 지지벡터머신은 현재 가장 많은 분야에서 적극적으로 사용되고 있는 테크닉입니다. 그 이유는 지지벡터 머신 특유의 안정성과 신뢰성, 그리고 다양성 때문입니다. 거의 모든 머신러닝의 과제(회귀와 분류)를 모두 뛰어난 수준으로 처리 가능하며, 여러가지 트릭을 사용해 비선형 모델에도 적용 가능합니다. 지도학습의 방법으로도 쓰이지만 지지벡터 클러스터링을 통해 비지도학습에도 사용될 수 있습니다. 로지스틱 회귀, 혹은 그와 관련된 퍼셉트론이나 딥러닝과 비교했을때도 구현하는데 훨씬 적은량의 노력이 필요하고 계산량도 크게 차이가 없습니다. 무엇보다 강한 장점은 높은 수준의 최적화를 기대할 수 있다는 점이지요. (이것은 SVM 이 Convex 형태의 최적화 모델을 가지고 있기 때문입니다.)

이러한 지지벡터 머신의 구조는 어느날 어느 수학자에게 단순하게 떠오른게 아닙니다. 그렇다보니 그 이론도 사실 그렇게 간단하지 않습니다. Vladimir N. Vapnik 이 지지벡터 머신을 생각해내는 그 절차적인 순서 흐름을 따라가야 이해가 되고 왜 지지벡터 머신이 우연이면서도 아름답고, 왜 강력한 머신러닝 도구가 되었는지를 이해할 수 있습니다.

우선 가장 기본적인 지지벡터 머신의 이론을 보도록 하겠습니다.

### 1. 판정방법 (Decision Rule)

![svm(unlicensed)](https://docs.opencv.org/2.4/_images/optimal-hyperplane.png)

분류기란 쉽게말해 임의의 객체(object)에 대한 여러 속성(properties)을 주어졌을때 그 객체를 하나의 항목(class)로 분류하거나 혹은 각각 항목에 속할 확률을 찾아내주는 매커니즘을 말합니다. 예를들어 위와 같은 그림의 데이터를 주어졌을때, 분류기의 역할은 임의의 점을 주어졌을 때 그 위치에 있는 객체는 빨간 내모인지 파란 동그라미일지를 찾아낸다고 보면 됩니다. 결정적인 방법이든 확률적인 방법이든 서로 다른 머신러닝 방법들은 각각 다른 분류 방법을 가지고 있습니다.

지지벡터 머신의 분류 방법은 하나의 선 (hyperplane) 을 기준으로 합니다. 이 선은 두 데이터셋을 가로질러 전체 공간을 이분하는 역할을 합니다. 지지벡터 머신이 제안하는 가장 최선의 이분법은, 두 데이터 사이에 공간, 혹은 마진이 최대한이 되는 이분법을 최적의 이분선으로 지향하고 있습니다. 위의 그림을 보면 최적의 선이 최대의 마진을 가질 수 있게 파란색과 빨간색을 이분하고 있습니다. 만약 주어진 새로운 점의 위치가 저 마진보다 위쪽에 위치한다면 파란색, 아래쪽에 위치한다면 빨간색이라 분별할 수 있습니다. (아직 그 사이에 점들은 생각하지 않겠습니다.)

만약 데이터셋이 이분할 수 없는 형태를 띄고 있을때를 비선형적 구조라 하는데, 이러한 비선형 모델로 전환하는 방법은 나중에 설명하겠습니다. 최선의 이분선에 유일성 또한 나중에 다루도록 하겠습니다.

이 판정 법칙을 수식으로 나타내보겠습니다. 지금까지는 저 선을 하나의 선형방정식으로 나타내었는데, 이번에는 조금 다른 방법을 사용해볼까 합니다. 비록 지금은 2차원 선상에 있지만, 다차원 상에 하이퍼평면을 쉽게 나타내는 방법중 하나는 수직 벡터를 찾는것입니다. (지지벡터라고 부르는 이유입니다.) 수직벡터와 하나의 위치 백터만 있다면 그 두 벡터로 저 선을 나타낼 수 있습니다.
