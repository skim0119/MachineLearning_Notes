# 지지벡터 머신(Support Vector Machine, SVM)

## 지지벡터 머신 역사와 이론

![svm(unlicensed)](https://docs.opencv.org/2.4/_images/optimal-hyperplane.png)

지지벡터 머신은 상당히 짧은 역사를 가지고 있습니다. 이론을 확립하고 제시한 사람이 아직도 살아계시니 말이죠. 그럼에도 불구하고 아마 지지벡터머신은 현재 가장 많은 분야에서 적극적으로 사용되고 있는 테크닉입니다. 그 이유는 지지벡터 머신 특유의 안정성과 신뢰성, 그리고 다양성 때문입니다. 거의 모든 머신러닝의 과제(회귀와 분류)를 모두 뛰어난 수준으로 처리 가능하며, 여러가지 트릭을 사용해 비선형 모델에도 적용 가능합니다. 지도학습의 방법으로도 쓰이지만 지지벡터 클러스터링을 통해 비지도학습에도 사용될 수 있습니다. 로지스틱 회귀, 혹은 그와 관련된 퍼셉트론이나 딥러닝과 비교했을때도 구현하는데 훨씬 적은량의 노력이 필요하고 계산량도 크게 차이가 없습니다. 무엇보다 강한 장점은 높은 수준의 최적화를 기대할 수 있다는 점이지요. (이것은 SVM 이 Convex 형태의 최적화 모델을 가지고 있기 때문입니다.)

이러한 지지벡터 머신의 구조는 어느날 어느 수학자에게 단순하게 떠오른게 아닙니다. 그렇다보니 그 이론도 사실 그렇게 간단하지 않습니다. Vladimir N. Vapnik 이 지지벡터 머신을 생각해내는 그 절차적인 순서 흐름을 따라가야 이해가 되고 왜 지지벡터 머신이 우연이면서도 아름답고, 왜 강력한 머신러닝 도구가 되었는지를 이해할 수 있습니다.

우선 가장 기본적인 지지벡터 머신의 이론을 보도록 하겠습니다.

### 1. 판정 (Decision Rule)

![svm(unlicensed)](https://docs.opencv.org/2.4/_images/optimal-hyperplane.png)

위와 같은 그림의 데이터를 주어졌을때, 분류기란 빨간 네모와 파란 동그라미를 가지고 임의의 점을 주었을때 둘중 어느 항목에 속하는지를 결정하는 절차입니다. 결정적인 방법이든 확률적인 방법이든 모든 머신러닝은 이 분류 방법을 가지고 시작합니다. 지지벡터 머신의 분류 방법은 위와 같이 하나의 선 (hyperplane)으로 두 데이터셋을 나누되, 그 기준이 파란색과 빨간색 사이에 간격(Margin)이 최대가 되게 나눕니다. 간단하게 말해, 저 마진보다 위쪽에 위치한다면 파란색, 아래쪽에 위치한다면 빨간색이라 생각할 수 있습니다. (아직 그 사이에 점들은 생각하지 않겠습니다.)

그렇다면 이 판정 법칙을 수식으로 나타내보겠습니다. 지금까지는 저 선을 하나의 선형방정식으로 나타내었는데, 이번에는 조금 다른 방법을 사용해볼까 합니다. 비록 지금은 2차원 선상에 있지만, 다차원 상에 하이퍼평면을 쉽게 나타내는 방법중 하나는 수직 벡터를 찾는것입니다. (지지벡터라고 부르는 이유입니다.) 수직벡터와 하나의 위치 백터만 있다면 그 두 벡터로 저 선을 나타낼 수 있습니다.
