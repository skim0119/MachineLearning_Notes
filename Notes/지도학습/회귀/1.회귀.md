회귀
====

나이브 베이스나 k-근접이웃 방법은 특징들만 비교해서 비슷한 항목을 구별해낸다고 볼 수 있습니다. 이러한 방법은 어떤 특징과 항목 사이에 특별한 관계가 없더라도 주어진 데이터를 분류해낸다는 강력한 장점이 있습니다. 하지만, 만약 명확하게 특징과 항목 사이에 규칙성이 있다면, 이러한 규칙을 활용해 조금 다른 방식의 분류를 할 수 있습니다. 그리고 이러한 규칙을 활용해 데이터의 속성과 분류가 얼마나 밀접하게 관련되있는지를 살펴볼 수 있습니다.

![iris image](images/irissepal.png)

회귀(Regression)이란 기본적으로 이러한 특징-결과 사이에 규칙성, 혹은 관계를 찾아내는데 쓰입니다.

가장 기본적인 회귀법인 선형회귀를 잠시 살펴본 뒤, 머신러닝에 사용되는 로지스틱 회귀를 보도록 하겠습니다.

선형회귀 이론
-------------

로지스틱회귀 이론
-----------------

장점
----

-	극한의 데이터에 대해 결과를 낼 수 있다.

단점
----

-	outlier, 이상치에 민감하다.
